{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkJGKyqxIwHw"
      },
      "source": [
        "### Sentiment Analysis on IMBd movie reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDAqw61QJGPk"
      },
      "source": [
        "Load in libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8_LKlvXI0VA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import tensorflow as tf\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh3hwrVj-uHw"
      },
      "source": [
        "### Load in datasets and create dataloader for NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e1tW3yN1WVx"
      },
      "outputs": [],
      "source": [
        "vocab_list = pd.read_csv('imdb.vocab', header = None)\n",
        "\n",
        "data2 = open(\"labeledBow.feat\", \"r\", encoding=\"utf8\")\n",
        "\n",
        "data_size = len(vocab_list)\n",
        "train = np.zeros(data_size)\n",
        "\n",
        "train_list = []\n",
        "train_label = []\n",
        "\n",
        "cnt = 0\n",
        "\n",
        "for line in data2:\n",
        "  line = line.strip()\n",
        "  indexes = line.split(\" \")\n",
        "  indexes.pop(0)\n",
        "\n",
        "  train = np.zeros(data_size + 2)\n",
        "  chance = np.random.uniform(0, 1)\n",
        "\n",
        "  if chance<=0.075:\n",
        "    for i in indexes:\n",
        "      pair = i.split(\":\")\n",
        "      idx = int(pair[0])\n",
        "      val = int(pair[1])\n",
        "\n",
        "      train[idx] = val\n",
        "\n",
        "    if cnt<=12499:\n",
        "      train_label.append([1,0]) #posivite\n",
        "    else:\n",
        "      train_label.append([0,1]) #negative\n",
        "        \n",
        "    train_list.append(train)\n",
        "\n",
        "  cnt+=1\n",
        "\n",
        "data2.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7zw1V1GMxaZ"
      },
      "outputs": [],
      "source": [
        "test_list = []\n",
        "test_label = []\n",
        "\n",
        "cnt = 0\n",
        "\n",
        "data2 = open(\"labeledBow.feat\", \"r\", encoding=\"utf8\")\n",
        "\n",
        "for line in data2:\n",
        "  line = line.strip()\n",
        "  indexes = line.split(\" \")\n",
        "  indexes.pop(0)\n",
        "\n",
        "  test = np.zeros(data_size)\n",
        "  chance = np.random.uniform(0, 1)\n",
        "\n",
        "  if chance<=0.1:\n",
        "    for i in indexes:\n",
        "      pair = i.split(\":\")\n",
        "      idx = int(pair[0])\n",
        "      val = int(pair[1])\n",
        "\n",
        "      test[idx] = val\n",
        "\n",
        "    if cnt<=12499:\n",
        "      test_label.append([1,0]) #posivite\n",
        "    else:\n",
        "      test_label.append([0,1]) #negative\n",
        "        \n",
        "    test_list.append(test)\n",
        "\n",
        "  cnt+=1\n",
        "\n",
        "data2.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "9rjkumypIu6Q",
        "outputId": "c124609c-6d81-488b-ae8c-3722b3ce1408"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-e251d289ce89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#train_label = np.array(train_label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtest_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtest_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_list' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "#train_list = np.array(train_list)\n",
        "#train_label = np.array(train_label)\n",
        "\n",
        "test_list = np.array(test_list)\n",
        "test_label = np.array(test_label)\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 32\n",
        "\n",
        "'''\n",
        "train_tensor_x = torch.Tensor(train_list) # transform to torch tensor\n",
        "train_tensor_y = torch.Tensor(train_label)\n",
        "train_dataset = TensorDataset(train_tensor_x, train_tensor_y) # create your datset\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "'''\n",
        "\n",
        "test_tensor_x = torch.Tensor(test_list) # transform to torch tensor\n",
        "test_tensor_y = torch.Tensor(test_label)\n",
        "test_dataset = TensorDataset(test_tensor_x, test_tensor_y) # create your datset\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oufk0dM06fq2"
      },
      "outputs": [],
      "source": [
        "class MLP1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(89527, 1024)\n",
        "        self.sig1 = nn.Sigmoid()\n",
        "\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.sig2 = nn.Sigmoid()\n",
        "\n",
        "        self.output = nn.Linear(512, 2)\n",
        "        self.sig3 = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        \n",
        "        x = self.fc1(x)\n",
        "        x = self.sig1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.sig2(x)\n",
        "\n",
        "        x = self.output(x)\n",
        "        x = self.sig3(x)\n",
        "\n",
        "        return x\n",
        "  \n",
        "mlp1 = MLP1()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrO5gwRcKEDA"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(mlp1.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "884twonlHQqj"
      },
      "source": [
        "### Trying to do multiple training sessions of random selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mE9ydeyHQCd",
        "outputId": "e3e56cc4-6c90-4720-b50a-c2051aa93c8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training session:  1\n",
            "[1,    10] loss: 0.714\n",
            "[1,    20] loss: 0.706\n",
            "[1,    30] loss: 0.686\n",
            "[1,    40] loss: 0.682\n",
            "[1,    50] loss: 0.651\n",
            "[1,    60] loss: 0.589\n",
            "[1,    70] loss: 0.549\n",
            "[2,    10] loss: 0.423\n",
            "[2,    20] loss: 0.405\n",
            "[2,    30] loss: 0.385\n",
            "[2,    40] loss: 0.386\n",
            "[2,    50] loss: 0.369\n",
            "[2,    60] loss: 0.376\n",
            "[2,    70] loss: 0.389\n",
            "[3,    10] loss: 0.337\n",
            "[3,    20] loss: 0.327\n",
            "[3,    30] loss: 0.336\n",
            "[3,    40] loss: 0.331\n",
            "[3,    50] loss: 0.337\n",
            "[3,    60] loss: 0.331\n",
            "[3,    70] loss: 0.328\n",
            "[4,    10] loss: 0.318\n",
            "[4,    20] loss: 0.332\n",
            "[4,    30] loss: 0.322\n",
            "[4,    40] loss: 0.322\n",
            "[4,    50] loss: 0.320\n",
            "[4,    60] loss: 0.317\n",
            "[4,    70] loss: 0.328\n",
            "[5,    10] loss: 0.326\n",
            "[5,    20] loss: 0.314\n",
            "[5,    30] loss: 0.320\n",
            "[5,    40] loss: 0.314\n",
            "[5,    50] loss: 0.319\n",
            "[5,    60] loss: 0.326\n",
            "[5,    70] loss: 0.318\n",
            "Starting training session:  2\n",
            "[1,    10] loss: 0.486\n",
            "[1,    20] loss: 0.443\n",
            "[1,    30] loss: 0.446\n",
            "[1,    40] loss: 0.418\n",
            "[1,    50] loss: 0.439\n",
            "[1,    60] loss: 0.476\n",
            "[1,    70] loss: 0.448\n",
            "[2,    10] loss: 0.381\n",
            "[2,    20] loss: 0.375\n",
            "[2,    30] loss: 0.389\n",
            "[2,    40] loss: 0.363\n",
            "[2,    50] loss: 0.353\n",
            "[2,    60] loss: 0.395\n",
            "[2,    70] loss: 0.353\n",
            "[3,    10] loss: 0.343\n",
            "[3,    20] loss: 0.343\n",
            "[3,    30] loss: 0.342\n",
            "[3,    40] loss: 0.364\n",
            "[3,    50] loss: 0.346\n",
            "[3,    60] loss: 0.359\n",
            "[3,    70] loss: 0.341\n",
            "[4,    10] loss: 0.327\n",
            "[4,    20] loss: 0.348\n",
            "[4,    30] loss: 0.326\n",
            "[4,    40] loss: 0.340\n",
            "[4,    50] loss: 0.323\n",
            "[4,    60] loss: 0.341\n",
            "[4,    70] loss: 0.356\n",
            "[5,    10] loss: 0.329\n",
            "[5,    20] loss: 0.333\n",
            "[5,    30] loss: 0.357\n",
            "[5,    40] loss: 0.323\n",
            "[5,    50] loss: 0.329\n",
            "[5,    60] loss: 0.350\n",
            "[5,    70] loss: 0.324\n",
            "Starting training session:  3\n",
            "[1,    10] loss: 0.431\n",
            "[1,    20] loss: 0.445\n",
            "[1,    30] loss: 0.446\n",
            "[1,    40] loss: 0.416\n",
            "[1,    50] loss: 0.455\n",
            "[1,    60] loss: 0.428\n",
            "[1,    70] loss: 0.456\n",
            "[1,    80] loss: 0.427\n",
            "[2,    10] loss: 0.369\n",
            "[2,    20] loss: 0.362\n",
            "[2,    30] loss: 0.376\n",
            "[2,    40] loss: 0.391\n",
            "[2,    50] loss: 0.369\n",
            "[2,    60] loss: 0.373\n",
            "[2,    70] loss: 0.366\n",
            "[2,    80] loss: 0.388\n",
            "[3,    10] loss: 0.353\n",
            "[3,    20] loss: 0.363\n",
            "[3,    30] loss: 0.356\n",
            "[3,    40] loss: 0.341\n",
            "[3,    50] loss: 0.355\n",
            "[3,    60] loss: 0.370\n",
            "[3,    70] loss: 0.358\n",
            "[3,    80] loss: 0.341\n",
            "[4,    10] loss: 0.345\n",
            "[4,    20] loss: 0.338\n",
            "[4,    30] loss: 0.331\n",
            "[4,    40] loss: 0.348\n",
            "[4,    50] loss: 0.351\n",
            "[4,    60] loss: 0.355\n",
            "[4,    70] loss: 0.361\n",
            "[4,    80] loss: 0.327\n",
            "[5,    10] loss: 0.348\n",
            "[5,    20] loss: 0.326\n",
            "[5,    30] loss: 0.338\n",
            "[5,    40] loss: 0.332\n",
            "[5,    50] loss: 0.341\n",
            "[5,    60] loss: 0.332\n",
            "[5,    70] loss: 0.332\n",
            "[5,    80] loss: 0.349\n",
            "Finished Training\n",
            "CPU times: user 32min 45s, sys: 1min 13s, total: 33min 59s\n",
            "Wall time: 33min 49s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 32\n",
        "\n",
        "vocab_list = pd.read_csv('imdb.vocab', header = None)\n",
        "\n",
        "for train_num in range(3):\n",
        "  \n",
        "  f = open(\"labeledBow.feat\", \"r\", encoding=\"utf8\")\n",
        "\n",
        "  data_size = len(vocab_list)\n",
        "  train = np.zeros(data_size)\n",
        "  train_list = []\n",
        "  train_label = []\n",
        "\n",
        "  cnt = 0\n",
        "\n",
        "  for line in f:\n",
        "    line = line.strip()\n",
        "    indexes = line.split(\" \")\n",
        "    indexes.pop(0)\n",
        "\n",
        "    train = np.zeros(data_size)\n",
        "    chance = np.random.uniform(0, 1)\n",
        "\n",
        "    if chance<=0.1: #randomly take 10% of the data set\n",
        "      for i in indexes:\n",
        "        pair = i.split(\":\")\n",
        "        idx = int(pair[0])\n",
        "        val = int(pair[1])\n",
        "\n",
        "        train[idx] = val\n",
        "\n",
        "      if cnt<=12499: #first half is positive reviews\n",
        "        train_label.append([1,0]) #posivite\n",
        "      else:\n",
        "        train_label.append([0,1]) #negative\n",
        "          \n",
        "      train_list.append(train)\n",
        "\n",
        "    cnt+=1\n",
        "\n",
        "  f.close()\n",
        "\n",
        "  train_list = np.array(train_list)\n",
        "\n",
        "  train_tensor_x = torch.Tensor(train_list) # transform to torch tensor\n",
        "  train_tensor_y = torch.Tensor(train_label)\n",
        "  train_dataset = TensorDataset(train_tensor_x, train_tensor_y) # create your datset\n",
        "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  print(\"Starting training session: \", train_num+1)\n",
        "\n",
        "  for epoch in range(5):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = mlp1(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 10 == 9:    # print every 10 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 10:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDDCP_JweQsD"
      },
      "source": [
        "### Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1BR4ir_eQOz",
        "outputId": "a3841366-942e-4264-9023-ef18ae0e0027"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model's state_dict:\n",
            "fc1.weight \t torch.Size([1024, 89527])\n",
            "fc1.bias \t torch.Size([1024])\n",
            "fc2.weight \t torch.Size([512, 1024])\n",
            "fc2.bias \t torch.Size([512])\n",
            "output.weight \t torch.Size([2, 512])\n",
            "output.bias \t torch.Size([2])\n"
          ]
        }
      ],
      "source": [
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in mlp1.state_dict():\n",
        "    print(param_tensor, \"\\t\", mlp1.state_dict()[param_tensor].size())\n",
        "\n",
        "torch.save(mlp1.state_dict(), 'NLP_model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhqzliGZKH8-",
        "outputId": "9c98d87a-3e41-4299-db9f-8f20016d1280"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,    10] loss: 0.700\n",
            "[1,    20] loss: 0.694\n",
            "[1,    30] loss: 0.681\n",
            "[1,    40] loss: 0.665\n",
            "[1,    50] loss: 0.614\n",
            "[2,    10] loss: 0.500\n",
            "[2,    20] loss: 0.424\n",
            "[2,    30] loss: 0.403\n",
            "[2,    40] loss: 0.380\n",
            "[2,    50] loss: 0.380\n",
            "[3,    10] loss: 0.340\n",
            "[3,    20] loss: 0.333\n",
            "[3,    30] loss: 0.327\n",
            "[3,    40] loss: 0.332\n",
            "[3,    50] loss: 0.325\n",
            "[4,    10] loss: 0.321\n",
            "[4,    20] loss: 0.315\n",
            "[4,    30] loss: 0.320\n",
            "[4,    40] loss: 0.321\n",
            "[4,    50] loss: 0.318\n",
            "[5,    10] loss: 0.317\n",
            "[5,    20] loss: 0.315\n",
            "[5,    30] loss: 0.315\n",
            "[5,    40] loss: 0.314\n",
            "[5,    50] loss: 0.318\n",
            "Finished Training\n",
            "CPU times: user 3min 32s, sys: 2.18 s, total: 3min 34s\n",
            "Wall time: 3min 34s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "for epoch in range(5):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = mlp1(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 10 == 9:    # print every 10 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 10:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJIQL6GdaQI2",
        "outputId": "30dccdc4-9dcc-4f29-b839-be4a91562273"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test reviews: 84.0 %\n"
          ]
        }
      ],
      "source": [
        "test_review = [[0]]\n",
        "test_label = [[0]]\n",
        "\n",
        "cnt = 0\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "data2 = open(\"labeledBow_test.feat\", \"r\", encoding=\"utf8\")\n",
        "\n",
        "for line in data2:\n",
        "  line = line.strip()\n",
        "  indexes = line.split(\" \")\n",
        "  indexes.pop(0)\n",
        "\n",
        "  test = np.zeros(data_size)\n",
        "  chance = np.random.uniform(0, 1)\n",
        "\n",
        "  for i in indexes:\n",
        "    pair = i.split(\":\")\n",
        "    idx = int(pair[0])\n",
        "    val = int(pair[1])\n",
        "\n",
        "    test[idx] = val\n",
        "\n",
        "  if cnt<=12499:\n",
        "    test_label[0] = [1,0] #posivite\n",
        "  else:\n",
        "    test_label[0] = [0,1] #negative\n",
        "      \n",
        "  test_review[0] = test\n",
        "\n",
        "  cnt+=1\n",
        "\n",
        "  test_review = np.array(test_review)\n",
        "  test_label = np.array(test_label)\n",
        "\n",
        "  # transform to torch tensor\n",
        "  features = torch.Tensor(test_review) \n",
        "  labels = torch.Tensor(test_label)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      # calculate outputs by running features through the network\n",
        "      outputs = mlp1(features)\n",
        "      # round the ouputs to make decision as whether review is pos or neg\n",
        "      predicted = torch.round(outputs.data)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "data2.close()\n",
        "\n",
        "acc = 100*(correct/2)//total\n",
        "print(f'Accuracy of the network on the test reviews: {100 * (correct/2) // total} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUb23OpcNrgp",
        "outputId": "3cb3e293-ebda-42fc-fef2-d6e3bc63e923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on thetest images: 87.0 %\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in test_dataloader:\n",
        "        review, labels = data\n",
        "        # calculate outputs by running features through the network\n",
        "        outputs = mlp1(review)\n",
        "        # round the ouputs to make decision as whether review is pos or neg\n",
        "        predicted = torch.round(outputs.data)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "mlp2_acc = 100*(correct/2)//total\n",
        "print(f'Accuracy of the network on the test reviews: {100 * (correct/2) // total} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwXDiqoPAjoV"
      },
      "source": [
        "## References:\n",
        "\n",
        "[1] PyTorch utils.Data: https://pytorch.org/docs/stable/data.html\n",
        "\n",
        "[2] PyTorch Training a classifier: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "\n",
        "[3] Pandas library: https://pandas.pydata.org/\n",
        "\n",
        "[4] Scikit-learn library: https://scikit-learn.org/stable/\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
